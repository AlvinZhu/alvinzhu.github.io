---
layout: post
mathjax: true
category: 深度学习
title: Mask R-CNN 论文翻译
tags: Mask_R-CNN 深度学习 目标检测
author: Alvin Zhu
date: 2017-10-07
---

* content
{:toc}

## 摘要

我们提出了一个概念上简单，灵活和通用的目标分割框架。我们的方法有效地检测图像中的目标，同时为每个实例生成高质量的分割掩码。称为Mask R-CNN的方法通过添加一个与现有目标检测框回归并行的，用于预测目标掩码的分支来扩展Faster R-CNN。Mask R-CNN训练简单，相对于Faster R-CNN，只需增加一个较小的开销，运行速度可达5 FPS。此外，Mask R-CNN很容易推广到其他任务，例如，允许我们在同一个框架中估计人的姿势。我们在COCO挑战的所有三个项目中取得了最佳成绩，包括目标分割，目标检测和人体关键点检测。在没有使用额外技巧的情况下，Mask R-CNN优于所有现有的单一模型，包括COCO 2016挑战优胜者。我们希望我们的简单而有效的方法将成为一个促进未来目标级识别领域研究的坚实基础。我们稍后将提供代码。






## 简介

目标检测和语义分割的效果在短时间内得到了很大的改善。在很大程度上，这些进步是由强大的基线系统驱动的，例如，分别用于目标检测和语义分割的Fast/Faster R-CNN[^12] [^34]和全卷积网络(FCN)[^29]框架。这些方法在概念上是直观的，提供灵活性和鲁棒性，以及快速的训练和推理。我们在这项工作中的目标是为目标分割开发一个相对有力的框架。

目标分割是具有挑战性的，因为它需要正确检测图像中的所有目标，同时也精确地分割每个目标。因此，它结合了来自经典计算机视觉任务目标检测的元素，其目的是对目标进行分类，并使用边界框定位每个目标，以及语义分割（通常来说，目标检测来使用边界框而不是掩码来标定每一个目标检测，而语义分割以在不区分目标的情况下表示每像素的分类。然而，目标分割既是语义分割，又是另一种形式的检测。）鉴于此，人们可能认为需要一种复杂的方法才能取得良好的效果。然而，我们的研究表明，使用非常简单，灵活和快速的系统就可以超越先前的最先进的目标分割结果。

我们称之为Mask R-CNN的方法通过添加一个用于在每个感兴趣区域（RoI）上预测分割掩码的分支来扩展Faster R-CNN [34]，这个分支与用于分类和目标检测框回归的分支并行执行，如下图（图1）所示（用于目标分割的Mask R-CNN框架）：

![Figure 1](/assets/2017-10-07-mask-r-cnn/figure1.png)

掩码分支是作用于每个RoI的小FCN，以像素到像素的方式预测分割掩码。Mask R-CNN易于实现和训练，它是基于Faster R-CNN这种灵活的框架的。此外，掩码分支只增加了很小的计算开销。

原理上，Mask R-CNN是Faster R-CNN的直接扩展，而要获得良好的结果，正确构建掩码分支至关重要。最重要的是，Faster R-CNN不是为网络输入和输出之间的像素到像素对齐而设计的。在《how RoIPool》 [^18] [^12]中提到，实际上，应用到目标上的核心操作执行的是粗略的空间量化特征提取。为了修正错位，我们提出了一个简单的，量化无关的层，称为RoIAlign，可以保留精确的空间位置。尽管是一个看似很小的变化，RoIAlign起到了很大的作用：它可以将掩码准确度提高10％至50％，在更严格的位置度量下显示出更大的收益。其次，我们发现解耦掩码和分类至关重要：我们为每个类独立地预测二进制掩码，这样不会跨类别竞争，并且依赖于网络的RoI分类分支来预测类别。相比之下，FCN通常执行每像素多类分类，分割和分类同时进行，基于我们的实验，对于目标分割效果不佳。

Mask R-CNN超越了COCO实例分割任务[28]上所有先前最先进的单一模型结果，其中包括COCO 2016挑战优胜者。作为副产品，我们的方法也优于COCO对象检测任务。在消融实验中，我们评估多个基本实例，这使我们能够证明其鲁棒性并分析核心因素的影响。

我们的模型可以在GPU上以200毫秒每帧的速度运行，使用一台有8个GPU的机器，在COCO上训练需要一到两天的时间。我们相信，快速的训练和测试速度，以及框架的灵活性和准确性将促进未来目标分割的研究。

最后，我们通过COCO关键点数据集上的人体姿态估计任务来展示我们框架的通用性[^28]。通过将每个关键点视为one-hot二进制掩码，只需要很少的修改，Mask R-CNN可以应用于人体关键点检测。不需要额外的技巧，Mask R-CNN超过了COCO 2016人体关键点检测比赛的冠军，同时运行速度可达5 FPS。因此，Mask R-CNN可以被更广泛地看作是用于目标级识别的灵活框架，并且可以容易地扩展到更复杂的任务。

我们将发布代码以促进未来的研究。

## 相关工作

**R-CNN：**R-CNN方法[^13]是通过找到一定数量的候选区域[^38] [^20]，并独立地在每个RoI上执行卷积[^25] [^24]来进行目标检测的。 基于R-CNN的改进[^18] [^12]，使用RoIPool在特征图上选取RoI，实现了更快的速度和更好的准确性。Faster R-CNN[^34]通过使用RPN学习注意机制来产生候选框。还有后续的对Faster R-CNN灵活性和鲁棒性的改进（例如[^35] [^27] [^21]）。这是目前在几个基准测试中领先的框架。

**目标分割：**在R- CNN的有效性的推动下，目标分割的许多方法都是基于segment proposals的。先前的方法[^13] [^15] [^16] [^9]依赖自下而上的分割[^38] [^2]。 DeepMask[^32]和[^33] [^8]通过学习提出分割候选，然后使用Fast R-CNN分类。在这些方法中，分割先于识别，这样做既慢又不太准确。同样，Dai等人[^10]提出了一个复杂的多级联级联，从候选框中预测候选分割，然后进行分类。相反，我们的方法并行进行掩码和类标签的预测，更简单也更灵活。

最近，Li等人[^26]将[^8]中的分割候选系统与[^11]中的目标检测系统进行了“全卷积目标分割”（FCIS）的融合。 在[^8] [^11] [^26]中的共同想法是用全卷积得到一组位置敏感的输出通道候选。这些通道同时处理目标分类，目标检测和掩码，这使系统速度变得更快。但FCIS在重叠实例上出现系统错误，并产生虚假边缘（图5）。

## Mask R-CNN

Mask R-CNN在概念上是简单的：Faster R-CNN为每个候选目标输出类标签和边框偏移量。为此，我们添加了一个输出目标掩码的第三个分支。因此，Mask R-CNN是一种自然而直观的点子。但是，附加的掩码输出与类和框输出不同，需要提取对象的更精细的空间布局。接下来，我们介绍Mask R-CNN的关键特点，包括像素到像素对齐，这是Fast/Faster R-CNN的主要缺失。

**Faster R-CNN：**我们首先简要回顾一下Faster R-CNN检测器[^34]。Faster R-CNN由两个阶段组成。称为区域提议网络（RPN）的第一阶段提出候选目标边界框。第二阶段，本质上是Fast R-CNN [^12]，使用RoIPool从每个候选框中提取特征，并进行分类和边界回归。两个阶段使用的特征可以共享，以便更快的推理。可以参考[^21]，了解Faster R-CNN和其他框架之间的最新综合比较。

**Mask R-CNN：**Mask R-CNN采用相同的两个阶段，具有相同的第一阶段（即RPN）。在第二阶段，与预测类和框偏移量并行，Mask R-CNN还为每个RoI输出二进制掩码。这与最近的其它系统相反，其分类取依赖于掩码预测（例如[^32] [^10] [^26]）。我们的方法遵循Fast R-CNN [12]，预测类和框偏移量并行（这在很大程度上简化了R-CNN的多级流水线[^13]）。

在训练期间，我们将在每个采样后的RoI上的多任务损失函数定义为$L = L_{cls} + L_{box} + L_{mask}$。分类损失$L_{cls}$和检测框损失$L_{box}$与[^12]中定义的相同。掩码分支对于每个RoI的输出维度为$Km^2$，即$K$个分辨率为$m \times m$的二进制掩码，每个类别一个，$K$表示类别数量。我们为每个像素应用Sigmoid，并将$L_{mask}$定义为平均二进制交叉熵损失。对于真实类别为$k$的RoI，仅在第$k$个掩码上计算$L_{mask}$（其他掩码输出不计入损失）。

我们对$L_{mask}$的定义允许网络为每个类独立地预测二进制掩码，这样不会跨类别竞争。我们依靠专用分类分支预测用于选择输出掩码的类标签。这将解耦掩码和类预测。这与通常将FCN [^29]应用于像素级Softmax和多重交叉熵损失的语义分段的做法不同。在这种情况下，掩码将在不同类别之间竞争。而我们的方法，使用了其它方法没有的像素级的Sigmod和二进制损失。我们通过实验发现，这种方法是改善目标分割效果的关键。

**掩码表示：**掩码表示输入目标的空间布局。因此，与通过全连接（fc）层不可避免地缩成短输出向量的类标签或框偏移不同，提取掩码的空间结构可以通过由卷积提供的像素到像素对应自然地被解决。

具体来说，我们使用FCN[^29]来为每个RoI预测一个$m \times m$的掩码。这允许掩码分支中的每个层显式的保持$m \times m$的对象空间布局，而不会将其缩成缺少空间维度的向量表示。与以前使用fc层掩码预测的的方法不同[^32] [^33] [^10]，我们的全卷积表示需要更少的参数，并且如实验所证明的更准确。

这种像素到像素的行为需要RoI特征，它们本身就是小特征图。为了更好地对齐，以准确地保留显式的像素空间对应关系，我们开发出在掩模预测中发挥关键作用的以下RoIAlign层。

**RoIAlign：**RoIPool[^12]是从每个RoI提取小特征图（例如，$7 \times 7$）的标准操作。 RoIPool首先将浮点数表示的RoI缩放到与特征图匹配的粒度，然后将缩放后的RoI分块，最后汇总每个块覆盖的区域的特征值（通常使用最大池化）。例如，对在连续坐标系上的$x$计算$[x / 16]$，其中16是特征图步幅，$[\cdot]$表示四舍五入。同样地，当对RoI分块时（例如$7 \times 7$）时也执行同样的计算。这样的计算使RoI与提取的特征错位。虽然这可能不会影响分类，因为分类对小幅度的变换具有一定的鲁棒性，但它对预测像素级精确的掩码有很大的负面影响。

为了解决这个问题，我们提出了一个RoIAlign层，可以去除RoIPool的错位，将提取的特征与输入准确对齐。我们提出的改变很简单：我们避免避免计算过程中的四舍五入（比如，我们使用$x / 16$代替$[x / 16]$）。我们选取分块中的4个常规的位置，使用双线性插值[^22]来计算每个位置的精确值，并将结果汇总（使用最大或平均池化）。（我们抽取四个常规位置，以便我们可以使用最大或平均池化。事实上，在每个分块中心取一个值（没有池化）几乎同样有效。我们也可以为每个块采样超过四个位置，我们发现这些位置的收益递减。）

如我们在[消融实验](#消融实验)中所示，RoIAlign的改进效果明显。我们还比较了[^10]中提出的RoIWarp操作。与RoIAlign不同，RoIWarp忽略了对齐问题，并在[^10]的实现中，有像RoIPool那样的四舍五入计算。因此，即使RoIWarp也采用[^22]提到的双线性重采样，如实验所示（更多细节见表格2c），它与RoIPool效果差不多。这表明了对齐起到了关键的作用。

**网络架构：**为了证明我们的方法的普适性，我们构造了多种不同结构的Mask R-CNN。详细来说就是，我们使用不同的：(i)用于整个图像上的特征提取的下层卷积网络，以及(ii)用于检测框识别（分类和回归）和掩码预测的上层网络。

我们使用"网络-深度-特征输出层"的方式命名底下层卷积网络。我们评估了深度为50或101层的ResNet[^19]和ResNeXt[^40]网络。使用ResNet[^19]的Faster R-CNN从第四阶段的最终卷积层提取特征，我们称之为C4。例如，使用ResNet-50的下层网络由ResNet-50-C4表示。这是[^19] [^10] [^21] [^36]中常用的选择。

我们还探讨了Lin等人 [^27]最近提出的另一种更有效的下层网络，称为特征金字塔网络（FPN）。 FPN使用具有横旁路连接的自顶向下架构，以从单尺度输入构建网络中的特征金字塔。使用FPN的Faster R-CNN根据其尺度提取不同级别的金字塔的RoI特征，不过其它部分和平常的ResNet类似。使用ResNet-FPN进行特征提取的Mask R-CNN可以在精度和速度方面获得极大的提升。有关FPN的更多细节，参见[^27]。

对于上层网络，我们基本遵循了以前论文中提出的架构，我们添加了一个全卷积的掩码预测分支。具体来说，我们扩展了 ResNet[^19]和FPN[^27]中提出的Faster R-CNN的上层网络。详细信息如下图（图3）所示：（上层网络架构：我们扩展了两种现有的Faster R-CNN上层网络架构[^19] [^27]，分别添加了一个掩码分支。图中数字表示分辨率和通道数，箭头表示卷积、反卷积或全连接层（可以通过上下文推断，卷积减小维度，反卷积增加维度。）所有的卷积都是$3 \times 3$的，除了输出层，是$1 \times 1$的。反卷积是$2 \times 2$的，步进为2，,我们在隐藏层中使用ReLU[^30]。左图中，“res5”表示ResNet的第五阶段，简单起见，我们修改了第一个卷积操作，使用$7 \times 7$，步长为1的RoI代替$14 \times 14$，步长为2的RoI[^19]。右图中的“$\times 4$”表示堆叠的4个连续的卷积。）

![Figure 3](/assets/2017-10-07-mask-r-cnn/figure3.png)

ResNet-C4的上层网络包括ResNet的第五阶段（即9层的“res5”[^19]），这是计算密集型的。对于FPN，下层网已经包含了res5，因此可以使上层网络包含更少的卷积核而变的更高效。

我们注意到我们的掩码分支是一个非常简单的结构。也许更复杂的设计有可能提高性能，但不是这项工作的重点。

### 实现细节

超参数的设置与现有的Fast/Faster R-CNN基本一致[^12] [^34] [^27]。虽然这些设定是在原始论文中是用于目标检测的[^12] [^34] [^27]，但是我们发现我们的目标分割系统也是可以用。

**训练：**与Faster R-CNN中的设置一样，如果RoI与真值框的IoU不小于0.5，则为正样本，否则为负样本。掩码损失函数$L_{mask}$仅在RoI的正样本上定义。掩码目标是RoI及其对应的真值框之间的交集的掩码。

我们采用以图像为中心的训练[^12]。图像被缩放（较短边）到800像素[^27]。批量大小为每个GPU2个图像，每个图像具有N个RoI采样，正负样本比例为1:3[^12]。 C4下层网络的N为64（如[^12] [^34]），FPN为512（如[^27]）。我们使用8个GPU训练（如此有效的批量大小为16）160k次迭代，学习率为0.02，在120k次迭代时学习率除以10。我们使用0.0001的权重衰减和0.9的动量。

RPN锚点跨越5个尺度和3个纵横比[^27]。为方便消融，RPN分开训练，不与Mask R-CNN共享特征。本文中的，RPN和Mask R-CNN具有相同的下层网络，因此它们是可共享的。

**测试：**在测试时，C4下层网络（如[^34]）中的候选数量为300，FPN为1000（如[^27]）。我们在这些候选上执行检测框预测分支，然后执行非极大值抑制[^14]。然后将掩码分支应用于评分最高100个检测框。尽管这与训练中使用的并行计算不同，但它可以加速推理并提高精度（由于使用更少，更准确的RoI）。掩码分支可以预测每个RoI的$K$个掩码，但是我们只使用第$k$个掩码，其中$k$是分类分支预测的类别。然后将$m \times m$浮点数掩码输出的大小调整为RoI大小，并使用阈值0.5将其二值化。

请注意，由于我们仅在前100个检测框中计算掩码，Mask R-CNN将边缘运行时间添加到其对应的Faster R-CNN版本（例如，相对约20％）。

## 实验：目标分割

我们对Mask R-CNN与现有技术进行彻底的比较，并且进行了综合的消融实验。我们的实验使用COCO数据集[^28]。我们报告标准的COCO指标，包括AP（平均超过IoU阈值），$AP_{50}$，$AP_{75}$和$AP_{S}$，$AP_{M}$，$AP_{L}$（不同尺度的AP）。除非另有说明，使用掩码IoU评估AP，这与[^5] [^27]一样。我们训练使用80k训练集和35k验证集的子集（trainval35k）的组合，并在剩下的5k个验证集子集（minival）上报告消融。我们还给出了test-dev [28]的结果，其没有公开的标签。本文发布后，我们将根据建议把在test-std测试的完整结果上传到公开排行榜。

### 主要结果

我们将Mask R-CNN与其它最先进的目标分割方法进行比较，如下表（表1）所示：（COCO test-dev上的目标分割掩码AP。 MNC[^10]和FCIS[^26]分别是COCO 2015和2016分割挑战的获胜者。Mask R-CNN优于更复杂的，包含多尺度训练和测试、水平翻转测试的FCIS+++，和OHEM[^35]。所有条目都是单模型的结果。）

![Table 1](/assets/2017-10-07-mask-r-cnn/table1.png)

我们的模型的所有实例都胜过了先前最先进的模型。此外，这些模型中的改进也同样可以应用到Mask R-CNN中。

Mask R-CNN的输出见下图（图2和图4）：

![Figure 2](/assets/2017-10-07-mask-r-cnn/figure2.png)

（图2）Mask R-CNN在COCO测试集上的结果。这些结果基于ResNet-101[^19]，掩码AP达到了35.7，并可以5FPS的速度运行。掩码标记为彩色，并且标记出了边框、类别和置信度。

![Figure 4](/assets/2017-10-07-mask-r-cnn/figure4.png)

（图4）Mask R-CNN在COCO测试集上更多的结果。使用ResNet-101-FPN，并可以35FPS运行。掩码AP为35.7（表1）。

Mask R-CNN取得了良好的效果。在下图（图5）中：

![Figure 5](/assets/2017-10-07-mask-r-cnn/figure5.png)

FCIS+++[^26]（上）对比 Mask R-CNN（下，ResNet-101-FPN）。 FCIS在重叠对象上有问题，Mask R-CNN没问题。

### 消融实验

我们进行了一些消融来分析Mask R-CNN。结果显示在下表(表2)中（Mask R-CNN的消融。我们在trainval35k上训练，在minival上测试，并报告掩码AP，除非另有说明。），并在下面详细讨论。

![Table 2](/assets/2017-10-07-mask-r-cnn/table2.png)

**结构：**表2a显示了具有各种使用不同下层网络的Mask R-CNN。它受益于更深层次的网络（50对比101）和高级设计，包括FPN和ResNeXt（我们使用$64 \times 4d$的普通的ResNeXt）。我们注意到并不是所有的框架都会从更深层次的或高级的网络中自动获益（参见[^21]中的基准测试）。

**独立与非独立掩码：**Mask R-CNN解耦了掩码和类预测：由于现有的检测框分支预测类标签，所以我们为每个类生成一个掩码，而不会在类之间产生竞争（通过像素级Sigmoid和二值化损失）。在表2b中，我们将其与使用像素级Softmax和非独立损失的方法进行比较（常用于FCN[^29]）。这些方法将掩码和类预测的任务结合，导致了掩码AP（5.5个点）的严重损失。这表明，一旦目标被归类（通过检测框分支），就可以预测二值化掩码而不用担心类别，这样可以使模型更容易训练。

**类相关与类无关掩码：**我们默认预测类相关的掩码，即每类一个$m \times m$掩码。有趣的是，这种方法与具有类别无关掩码的Mask R-CNN（即，预测单个$m \times m$输出而不论是那一类）几乎同样有效：对于ResNet-50-C4掩码AP为29.7，而对于类相关的对应的模型AP为30.3 。这进一步突出了我们的方法中的改进：解耦了分类和分割。

**RoIAlign：**表2c显示了对我们提出的RoIAlign层的评估。对于这个实验，我们使用的下层网络为ResNet-50-C4，其步进为16。RoIAlign相对RoIPool将AP提高了约3个点，在高IoU（$AP_{75}$）结果中增益更多。 RoIAlign对最大/平均池化不敏感，我们在本文的其余部分使用平均池化。

此外，我们与采用双线性采样的MNC [10]中提出的RoIWarp进行比较。如[实验：目标分割](#实验：目标分割)所述，RoIWarp仍然四舍五入了RoI，与输入失去了对齐。从表2c可以看出，RoIWarp与RoIPool效果差不多，比RoIAlign差得多。这突出表明正确的对齐是关键。

我们还使用ResNet-50-C5下层网络评估了RoIAlign，其步进更大，达到了32像素。我们使用与图3（右）相同的上层网络，因为res5不适用。表2d显示，RoIAlign将掩码AP提高了7.3个点，并将掩码的$AP_{75}$ 提高了10.5个点（相对改善了50％）。此外，我们注意到，与RoIAlign一样，使用步幅为32的C5特征（30.9 AP）比使用步幅为16的C4特征（30.3 AP，表2c）更加精准。 RoIAlign在很大程度上解决了使用大步进特征进行检测和分割的长期挑战。

最后，当与FPN一起使用时，RoIAlign显示出1.5个掩码AP和0.5个检测框AP的增益，FPN具有更精细的多级步长。对于需要更精细对准的关键点检测，即使使用FPN，RoIAlign也显示出很大的增益（表6）。

**掩码分支：**分割是一个像素到像素的任务，我们使用FCN来利用掩码的空间布局。在表2e中，我们使用ResNet-50-FPN下层网络来比较多层感知机（MLP）和FCN。使用FCN可以提供超过MLP 2.1个点的AP增益。为了与与MLP进行公平的比较，FCN的上层网络的卷积层没有被预训练。

### 目标检测结果

我们在COCO数据集上将Mask R-CNN与其它最先进的目标检测方法进行比较，如下表（表3）所示：（目标检测结果（目标边界框AP），单模型，在test-dev上与其它最先进的技术对比。使用ResNet-101-FPN的Mask R-CNN优于所有先前最先进的模型的基本变体（实验中忽略了掩码输出）。Mask R-CNN超过[^27]的增益来自使用RoIAlign（+1.1 $AP^{bb}$），多任务训练（+0.9 $AP^{bb}$）和ResNeXt-101（+1.6 $AP^{bb}$）。）

![Table 3](/assets/2017-10-07-mask-r-cnn/table3.png)

对于该结果，虽然完整的Mask R-CNN模型被训练，但是测试时仅使用分类和检测的输出（忽略掩码输出）。Mask R-CNN使用ResNet-101- FPN优于所有先前最先进的模型的基本变体，包括单模型的G-RMI的[^21]，COCO 2016目标检测挑战的获胜者。使用ResNeXt-101-FPN的Mask R-CNN进一步改善了结果，其AP相对于使用单模型的前最佳结果[^36]（使用Inception-ResNet-v2-TDM） 提升了3个点。

作为进一步的比较，我们训练了一个没有掩码分支版本的Mask R-CNN，见表3中的“Faster R-CNN，RoIAlign”。由于RoIAlign，该模型的性能优于[^27]中提出的模型。但是，比Mask R-CNN低0.9个点的AP。这个差距这是由于Mask R-CNN的多任务训练产生的。

最后，我们注意到，Mask R-CNN在其掩码和检测框的AP之间的差距很小：例如，AP 37.1（掩码，表1）与AP 39.8（检测框，表3）之间的差距仅2.7个点。这表明我们的方法在很大程度上弥补了目标检测与更具挑战性的目标分割任务之间的差距。

### 速度

**测试：**我们训练一个ResNet-101-FPN模型，在RPN和Mask R-CNN阶段之间共享特征，遵循Faster R-CNN的四阶段训练[^34]。该模型在Nvidia Tesla M40 GPU上处理每个图像需要195ms（加上15毫秒的CPU时间，用于将输出的大小调整到原始分辨率），并且达到了与非共享特征模型相同的掩码AP。我们还指出，ResNet-101-C4变体需要大约400ms，因为它的上层模型比较复杂（图3），所以我们不建议在实践中使用C4变体。

虽然Mask R-CNN很快，但我们注意到，我们的设计并没有针对速度进行优化，[^21]可以实现更好的速度/精度平衡，例如，通过改变图像尺寸和候选数量，这超出了本文的范围。

**训练：**Mask R-CNN的训练也很快。在COCO trainval35k上使用ResNet-50-FPN进行训练，我们的同步8 GPU实现（每个批次耗时0.72秒，包含16个图像）需要32小时，而ResNet-101-FPN需要44小时。事实上，快速原型可以在不到一天的时间内在训练集上进行训练。我们希望这样快速的训练将会消除这一领域的重大障碍，并鼓励更多的人对这个具有挑战性的课题进行研究。

## Mask R-CNN人体姿态估计

我们的框架可以很容易地扩展到人类姿态估计。我们将关键点的位置建模为one-hot掩码，并采用Mask R-CNN来预测$K$个掩码，每个对应$K$种关键点类型之一（例如左肩，右肘）。此任务有助于展示Mask R-CNN的灵活性。

我们注意到，我们的系统利用了人类姿态的最小领域知识，因为实验主要是为了证明Mask R-CNN框架的一般性。我们期望领域知识（例如，建模结构[^6]）将是我们简单方法的补充，但这超出了本文的范围。

**实现细节：**在适配关键点时，我们对分割系统进行细微的修改。对于目标的$K$个关键点中的每一个，训练目标是一个one-hot的$m \times m$二进制掩码，其中只有一个像素被标记为前景。在训练期间，对于每个可视的关键点真实值，我们最小化在$m^2$路Softmax输出上的交叉熵损失（这驱使一个点被检测到）。我们注意到，和目标分割一样，K个关键点的检测仍然是独立对待的。

我们采用ResNet-FPN的变体，关键点检测的上层架构类似于图3（右图），由八个堆叠的$3 \times 3$ 512-d卷积层，后面是一个反卷积层进行$2\times$双线性上采样，产生分辨率$56 \times 56$的输出。我们发现相对较高的分辨率输出(与掩码相比）是关键点级精确定位所必需的。

我们使用包含关键点标注的COCO trainval35k图像训练模型。由于训练集较小，为了减少过拟合，我们训练时将图像在$[640, 800]$范围内随机缩放，测试则统一缩放到800像素。我们的训练迭代90k次，从0.02的学习率开始，并在迭代次数达到60k和80k次时将学习率除以10。检测框的非极大值抑制阈值为0.5。其他实现细节与[实现细节](#实现细节)相同。

**人体姿态估计实验：**使用ResNet-50-FPN评估人体关键点的AP（$AP^{kp}$）。我们也尝试了ResNet-101，不过效果差不多，可能是因为更深层次的模型需要更多的训练数据，但是这个数据集相对较小。

如下表（表4）所示：（COCO test-dev 上的关键点检测AP。我们的（ResNet-50-FPN）模型是以5 FPS的速度运行的单模型。 CMU-Pose+++[^6]是2016年度的优胜者，使用多尺度测试，CPM进行后处理[^39]，并使用目标检测进行过滤，提高了约5个点（与作者沟通确认）。 †：G-RMI使用两种模型（Inception-ResNet-v2 + ResNet-101），用COCO加MPII[^1]（25k图像）进行训练。由于他们使用了更多的数据，无法直接与Mask R-CNN进行比较。）

![Table 4](/assets/2017-10-07-mask-r-cnn/table4.png)

我们的结果（62.7 $AP^{kp}$）比使用多级处理流水线的COCO 2016关键点检测获胜者[^6]高出0.9个点。我们的方法要简单得多，速度更快。

更重要的是，我们用一个统一的模型，可以5 FPS的速度同时做目标检测、目标分割和关键点检测。添加目标分割分支（针对人员类别）将test-dev上的$AP^{kp}$提升到63.1（表4）。更多在minival上的多任务学习的消除在下表（表5）中：（目标检测、目标分割和关键点检测的多任务学习，在minival上的测试。为了公平起见，所有的模型都使用相同的训练数据。下层网络是ResNet-50-FPN。 第三行在minival上64.2 AP，在test-dev上62.7 AP。第四行在minival上64.7 AP，在test-dev上有63.1 AP，见表4。）

![Table 5](/assets/2017-10-07-mask-r-cnn/table5.png)

将掩码分支添加到仅做目标检测（如，Faster R-CNN）或仅检测关键点的网络上也会改进这些任务的准确率。然而，添加关键点分支会轻微降低目标检测/目标分割的AP，关键点检测会从多任务训练中获益，但它不会改善其他任务的准确率。然而，共同学习所有三个任务可以使统一的系统同时有效地预测所有输出，如下图（图6）所示：（使用Mask R-CNN（ResNet-50-FPN）在COCO test上的关键点检测结果，该模型也同时输出目标分割结果。其关键点检测的AP为63.1，运行速度为5 FPS。）。

![Figure 6](/assets/2017-10-07-mask-r-cnn/figure6.png)

我们还调查了RoIAlign对关键点检测的影响，如下表（表6）所示：（RoIAlign与RoIPool在minival上关键点检测。）

![Table 6](/assets/2017-10-07-mask-r-cnn/table6.png)

尽管这款ResNet-50-FPN下层网络有较小的步进（例如，最小步进为4像素），但RoIAlign相对RoIPool仍然有明显的改进，并将$AP_{kp}$提高了4.4个点。这是因为关键点检测对定位精度更敏感。这再次表明对齐对像素级定位是至关重要的，包括掩码和关键点。

鉴于Mask R-CNN提取目标框，掩码和关键点的有效性，我们期望它也可以成为其它目标级任务的有效框架。


## Cityscapes上的实验

我们进一步报告Cityscapes [7]数据集的目标分割结果。该数据集具有精细标注的2975个训练图像，500个验证图像和1525个测试图像。它还有20k粗糙的训练图像，无精细标注，我们不使用它们。所有图像的分辨率为2048 x 1024像素。目标分割任务涉及8个对象类别，其训练集中的目标数为：

| 人     | 骑手   | 小汽车   | 卡车   | 公交车  | 火车   | 摩托车  | 自行车  |
| ----- | ---- | ----- | ---- | ---- | ---- | ---- | ---- |
| 17.9k | 1.8k | 26.9k | 0.5k | 0.4k | 0.2k | 0.7k | 3.7k |

该任务的目标分割性能由和COCO一样的掩码AP（在IoU阈值上平均）来测量，也包括$AP_{50}$（即，IoU为0.5的掩码AP）。

**实现：**我们Mask R-CNN模型使用的下层网络是ResNet-FPN-50，我们也测试了对应的101层的网络，不过由于数据集比较小，性能相似。我们将图像在$[800， 1024]$像素范围内随机缩放（较短边）进行训练，从而减少过拟合。测试时则统一缩放到1024像素。我们使用的批量大小为每个GPU 1个图像（实际上8个GPU上有8个），学习率为0.01，迭代次数为24k，在迭代次数达到18k时，学习率减少到0.001。其他实现细节与[实现细节](#实现细节)相同。

**结果：**我们在测试集和验证集上，将我们的结果与其它主流方法进行了比较，如下表（表7）所示：

![Table 7](/assets/2017-10-07-mask-r-cnn/table7.png)

在不使用无精细标注的训练集的情况下，我们的方法在测试集上的AP达到了26.2，相对于以前的最佳结果（使用了所有的训练集），相对提升了超过30％。与仅使用精细标注训练集（17.4 AP）的前最佳结果相比，相对提升了约50％。在一台8 GPU的机器上需要约4个小时的训练才能获得此结果。

对于人和小汽车类别，Cityscapes数据集包含了大量的类内重叠目标（每个图像平均6人和9辆小汽车）。我们认为类内重叠是目标分割的核心难点。我们的方法在这两个类别相对前最佳结果有大幅度改善（人相对提升了约85％，从16.5提高到30.5，小汽车相对提升了约30％，从35.7提高到46.9）。

Cityscapes数据集的主要挑战是训练数据较少，特别是对于卡车，公共汽车和火车的类别，每个类别的训练样本大约有200-500个。为了在一定程度上改善这个问题，我们进一步报告了使用COCO预训练的结果。为了做到这一点，我们使用预先训练好的COCO Mask R-CNN模型（骑手类别被随机初始化）。然后我们在Cityscapes数据集上进行4k次迭代来微调这个模型，其中学习速率在迭代次数达到3k时减少，微调需要约1小时。

使用COCO预训练的Mask R-CNN模型在测试集上达到了32.0 AP，比不预训练的模型提高了6个点。这表明足够的训练数据的重要性。同时，在Cityscapes数据集上的目标分割还收到其low-shot学习性能的影响。我们发现，使用COCO预训练是减轻涉及此数据集的数据数据偏少问题的有效策略。

最后，我们观察到测试集和训练集AP之间的偏差，从[^23] [^4]的结果也可以看出。我们发现这种偏差主要是由卡车，公共汽车和火车类别造成的，其中只使用精细标注训练数据的模型，在验证集和测试集上的AP分别为28.8/22.8，53.5/32.2和33.0/18.6。这表明这些训练数据很少的类别存在domain shift。 COCO预训练有助于改善这些类别上的结果，然而，domain shift依然存在，在验证集和测试集上的AP分别为38.0/30.1，57.5/40.9和41.2/30.9。不过，对于人和小汽车类别，我们没有看到任何此类偏差（在验证集和测试集上的AP偏差在±1以内）。

Cityscapes的结果示例如下图（图7）所示：（Mask R-CNN在Cityscapes的测试结果（32.0 AP）。右下图出错。）

![Figure 7](/assets/2017-10-07-mask-r-cnn/figure7.png)

# 参考文献

[^1]: M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2D human pose estimation: New benchmark and state of the art analysis. In CVPR, 2014.
[^2]: P. Arbeláez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping. In CVPR, 2014.
[^3]: A. Arnab and P. H. Torr. Pixelwise instance segmentation with a dynamically instantiated network. In CVPR, 2017.
[^4]: M. Bai and R. Urtasun. Deep watershed transform for instance segmentation. In CVPR, 2017.
[^5]: S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016.
[^6]: Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In CVPR, 2017.
[^7]: M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.
[^8]: J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive fully convolutional networks. In ECCV, 2016.
[^9]: J. Dai, K. He, and J. Sun. Convolutional feature masking for joint object and stuff segmentation. In CVPR, 2015.
[^10]: J. Dai, K. He, and J. Sun. Instance-aware semantic segmentation via multi-task network cascades. In CVPR, 2016.
[^11]: J. Dai, Y. Li, K. He, and J. Sun. R-FCN: Object detection via region-based fully convolutional networks. In NIPS, 2016.
[^12]: R. Girshick. Fast R-CNN. In ICCV, 2015.
[^13]: R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.
[^14]: R. Girshick, F. Iandola, T. Darrell, and J. Malik. Deformable part models are convolutional neural networks. In CVPR, 2015.
[^15]: B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In ECCV. 2014.
[^16]: B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Hyper-columns for object segmentation and fine-grained localization. In CVPR, 2015.
[^17]: Z. Hayder, X. He, and M. Salzmann. Shape-aware instance segmentation. In CVPR, 2017.
[^18]: K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.
[^19]: K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
[^20]: J. Hosang, R. Benenson, P. Dollár, and B. Schiele. What makes for effective detection proposals? PAMI, 2015.
[^21]: J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. In CVPR, 2017.
[^22]: M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial transformer networks. In NIPS, 2015.
[^23]: A. Kirillov, E. Levinkov, B. Andres, B. Savchynskyy, and C. Rother. Instancecut: from edges to instances with multicut. In CVPR, 2017.
[^24]: A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.
[^25]: Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.
[^26]: Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei. Fully convolutional instance-aware semantic segmentation. In CVPR, 2017.
[^27]: T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In CVPR, 2017.
[^28]: T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.
[^29]: J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
[^30]: V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.
[^31]: G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler, and K. Murphy. Towards accurate multi-person pose estimation in the wild. In CVPR, 2017.
[^32]: P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to segment object candidates. In NIPS, 2015.
[^33]: P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár. Learning to refine object segments. In ECCV, 2016.
[^34]: S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.
[^35]: A. Shrivastava, A. Gupta, and R. Girshick. Training regionbased object detectors with online hard example mining. In CVPR, 2016.
[^36]: A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Beyond skip connections: Top-down modulation for object detection. arXiv:1612.06851, 2016.
[^37]: C. Szegedy, S. Ioffe, and V. Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. In ICLR Workshop, 2016.
[^38]: J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.
[^39]: S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Convolutional pose machines. In CVPR, 2016.
[^40]: S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In CVPR, 2017.
